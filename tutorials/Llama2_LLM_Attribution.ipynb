{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdd6ea2",
   "metadata": {},
   "source": [
    "# Understanding Llama2 with Captum LLM Attribution\n",
    "\n",
    "In this tutorial, we will demonstrate the LLM attribution functionality introduced in Captum v0.7, which makes it a breeze to applying the attribution algorithms to interpret the large langague models (LLM) in text generation. The new functionalities include a series utilities that help you with many common tedious scaffolding required by LLMs like defining intepretable features in text input and handling the sequential predictions. You can also check our paper for more details https://arxiv.org/abs/2312.05491\n",
    "\n",
    "Next, we will use Llama2 (7b-chat) as an example and use both perturbation-based and grandient-based algrithms respectively to see how the input prompts lead to the generated content. First, let's import the needed dependencies. Specifically, from Captum, besides the algorithms `FeatureAblation` and `LayerIntegratedGradients` themselves, we will also import:\n",
    "- perturbation-based and gradient-based wrappers for LLM, `LLMAttribution` and `LLMGradientAttribution`\n",
    "- text-based interpretable input adapters, `TextTokenInput` and `TextTemplateInput`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inside-current",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/craymichael/.conda/envs/captum/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so\n",
      "False\n",
      "CUDA SETUP: CUDA runtime path found: /home/craymichael/.conda/envs/captum/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Required library version not found: libbitsandbytes_cuda113_nocublaslt.so. Maybe you need to compile it from source?\n",
      "CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n",
      "\n",
      "================================================ERROR=====================================\n",
      "CUDA SETUP: CUDA detection failed! Possible reasons:\n",
      "1. CUDA driver not installed\n",
      "2. CUDA not installed\n",
      "3. You have multiple conflicting CUDA libraries\n",
      "4. Required library not pre-compiled for this bitsandbytes release!\n",
      "CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.\n",
      "CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n",
      "================================================================================\n",
      "\n",
      "CUDA SETUP: Something unexpected happened. Please compile from source:\n",
      "git clone git@github.com:TimDettmers/bitsandbytes.git\n",
      "cd bitsandbytes\n",
      "CUDA_VERSION=113 make cuda11x_nomatmul\n",
      "python setup.py install\n",
      "CUDA SETUP: Setup Failed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/craymichael/.conda/envs/captum/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/craymichael/.conda/envs/captum/lib/libcudart.so'), PosixPath('/home/craymichael/.conda/envs/captum/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/home/craymichael/.conda/envs/captum/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
      "File \u001b[0;32m~/.conda/envs/captum/lib/python3.9/site-packages/bitsandbytes/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     MatmulLtState,\n\u001b[1;32m      9\u001b[0m     bmm_cublas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     matmul_4bit\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n",
      "File \u001b[0;32m~/.conda/envs/captum/lib/python3.9/site-packages/bitsandbytes/research/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     switchback_bnb,\n\u001b[1;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[1;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/captum/lib/python3.9/site-packages/bitsandbytes/research/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001b[0;32m~/.conda/envs/captum/lib/python3.9/site-packages/bitsandbytes/research/nn/modules.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, device, dtype, nn\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalOptimManager\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n\u001b[1;32m     11\u001b[0m T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.nn.Module\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/captum/lib/python3.9/site-packages/bitsandbytes/optim/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madagrad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam, Adam8bit, Adam32bit, PagedAdam, PagedAdam8bit, PagedAdam32bit\n",
      "File \u001b[0;32m~/.conda/envs/captum/lib/python3.9/site-packages/bitsandbytes/cextension.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mgenerate_instructions()\n\u001b[1;32m     19\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mprint_log_stack()\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    python -m bitsandbytes\u001b[39m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m     28\u001b[0m lib\u001b[38;5;241m.\u001b[39mcadam32bit_grad_fp32 \u001b[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n\u001b[1;32m     29\u001b[0m lib\u001b[38;5;241m.\u001b[39mget_context\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mc_void_p\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from captum.attr import (\n",
    "    FeatureAblation, \n",
    "    ShapleyValues,\n",
    "    LayerIntegratedGradients, \n",
    "    LLMAttribution, \n",
    "    LLMGradientAttribution, \n",
    "    TextTokenInput, \n",
    "    TextTemplateInput,\n",
    "    ProductBaselines,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2695ee",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Let's make a helper function to load models through Huggingface. We will also add an extra step for 4-bits quantization which can effectively reduce the GPU memory consumption. Now, we can use them to load \"Llama-2-7b-chat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = \"10000MB\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\" \n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f41636-55e3-4bbd-acf9-e7e662f66365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/craymichael/.conda/envs/captum/lib/python3.9/site-packages/bitsandbytes/__init__.py'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "\n",
    "if '_henlo' not in locals():\n",
    "    _henlo = importlib.metadata.version\n",
    "importlib.metadata.version = lambda n, *a, **kw: _henlo(n.replace('bitsandbytes', 'bitsandbytes-cuda113'), *a, **kw)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.meta_path\n",
    "\n",
    "import bitsandbytes as bb\n",
    "\n",
    "bb.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-uncertainty",
   "metadata": {},
   "source": [
    "Let's test the model with a simple prompt and take a look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"Dave lives in Palm Coast, FL and is a lawyer. His personal interests include\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(model_input[\"input_ids\"], max_new_tokens=15)[0]\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-insight",
   "metadata": {},
   "source": [
    "## Perturbation-based Attribution\n",
    "\n",
    "OK now, the model is working and has completed the given prompt by producing several possible interests. To understand how the model produces them based on the prompt, we will first use the perturbation-based algrotihms from Captum to understand the generation. We can start with the simplest `FeatureAblation`, which ablates each of the features of this string to see how it affects the predicted probability of the target string. The way is the same as before: feed the model into the corresponding constructor to initiate the attribution method. But additionally, to help it work with text-based input and output, we need to wrap it with the new `LLMAttribution` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FeatureAblation(model)\n",
    "\n",
    "llm_attr = LLMAttribution(fa, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d2a02",
   "metadata": {},
   "source": [
    "The newly created `llm_attr` is the same as the wrapped attribution method instance which provides an `.attribute()` function taking the model inputs and returns the attribution scores of cared features within the inputs. However, by default, Captum's attribution algorithms assume each input into the model must be PyTorch tensors and perturb them at tensor level. This is likely not what we want for LLM, where we are more interested in the interpretable text input and making text modifications like removing a text segment, than a less meaningful tensor of token indices. To solve this, we introduce a new adapter design called `InterpretableInput` which handles the conversion between more interpretable input type and tensor, and tells Captum how to work with them. `llm_attr` is made to accept certain text-based `InterpretableInput` as the arguements. The concept of \"Interpretable Input\" mainly comes from the following two papers:\n",
    "- [“Why Should I Trust You?”: Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)\n",
    "- [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)\n",
    "\n",
    "The question now is what are the intepretable features we want to understand in text. One most common and straightforward answer is \"tokens\". And we provide `TextTokenInput` specifically for such use cases. `TextTokenInput` is an `InterpretableInput` for text whose interpretable features are the tokens with respect to a given tokenizer. So let's create one and calculate its attribution w.r.t the previous generated output as the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTokenInput(\n",
    "    eval_prompt, \n",
    "    tokenizer,\n",
    "    skip_tokens=[1],  # skip the special token for the start of the text <s>\n",
    ")\n",
    "\n",
    "target = \"playing guitar, hiking, and spending time with his family.\"\n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53921fcb",
   "metadata": {},
   "source": [
    "With just a few lines of codes, we now get the `FeatureAblation` attribution result of our LLM. The return contains the attribution tensors to both the entire generated target seqeuence and each generated token, which tell us how each input token impact the output and each token within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"attr to the output sequence:\", attr_res.seq_attr.shape)  # shape(n_input_token)\n",
    "print(\"attr to the output tokens:\", attr_res.token_attr.shape)  # shape(n_output_token, n_input_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfb8f1",
   "metadata": {},
   "source": [
    "It also provides the utilities to visualize the results. Next we will plot the token attribution to view the relations between input and output tokens. As we will see, the result is generally very positive. This is expected, since the target, \"playing guitar, hiking, and spending time with his family\", is what the model feel confident to generate by itself given the input tokens. So change in the input is more likely divert the model from this target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebdd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f039697",
   "metadata": {},
   "source": [
    "However, it may not always make sense to define individual token as intepretable features and perturb it. Tokenizers used in modern LLMs may break a single word making the tokens not intepretable by themselves. For example, in our case above, the tokenizer can break the word \"Palm\" into \"_Pal\" and \"m\". It doesn't make much sense to study the separate attribution of them. Moreover, even a whole word can be meaningless. For example, \"Palm Coast\" together result in a city name. Changing just partial of its tokens would likely not give anything belongs to the natural distribution of potential cities in Florida, which may lead to unexpected impacts on the perturbed model output.\n",
    "\n",
    "Therefore, Captum offers another more customizable interpretable input class, `TextTemplateInput`, whose interpretable features are certain segments (e.g., words, phrases) of the text defined by the users. For instance, our prompt above contains information about name, city, state, occupation, and pronoun. Let's define them as the interpretable features to get their attribution. \n",
    "\n",
    "The target to interpret can be any potential generations that we are interested in. Next, we will customize the target to something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTemplateInput(\n",
    "    template=\"{} lives in {}, {} and is a {}. {} personal interests include\", \n",
    "    values=[\"Dave\", \"Palm Coast\", \"FL\", \"lawyer\", \"His\"],\n",
    ")\n",
    "\n",
    "target = \"playing golf, hiking, and cooking.\"\n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target)\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56535322",
   "metadata": {},
   "source": [
    "We know that perturbation-based algrotihms calculate the attribution by switching the features between \"presence\" and \"absence\" states. So what should a text feature look like here when it is in \"absence\" in the above example? Captum allows users to set the baselines, i.e., the reference values, to use when a feature is absent. By default, `TextTemplateInput` uses empty string `''` as the baselines for all, which is equivalent to the removal of the segments. This may not be perfect for the same out-of-distribution reason. For example, when the feature \"name\" is absent, the prompt loses its subjective and no longer makes much sense. \n",
    "\n",
    "To improve it, let's manually set the baselines to something that still fit the context of the original text and keep it within the natural data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTemplateInput(\n",
    "    template=\"{} lives in {}, {} and is a {}. {} personal interests include\", \n",
    "    values=[\"Dave\", \"Palm Coast\", \"FL\", \"lawyer\", \"His\"],\n",
    "    baselines=[\"Sarah\", \"Seattle\", \"WA\", \"doctor\", \"Her\"],\n",
    ")\n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target)\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f5712",
   "metadata": {},
   "source": [
    "The result represents how the features impacts the output compared with the single baseline. It can be a useful setup to have some interesting findings. For example, the city name \"Palm Coast\" is more positive to \"playing golf\" but negative to \"hiking\" compared with \"Seattle\".\n",
    "\n",
    "But more generally, we would prefer a distribution of baselines so the attribution method will sample from for generosity. Here, we can leverage the `ProductBaselines` to define a Cartesian product of different baselines values of various features. And we can specify `num_trials` in attribute to average over multiple trials\n",
    "\n",
    "Another issue we notice from the above results is that there are correlated aspects of the prompt which should be ablated together to ensure that the input remain in distribution, e.g. Palm Coast, FL should be ablated with Seattle, WA. We can accomplish this using a mask as defined below, which will group (city, state) and (name, pronoun). `TextTemplateFeature` accepts the argument `mask` allowing us to set the group indices. To make it more explicit, we can also define the template and its values in dictionary format instead of list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = ProductBaselines(\n",
    "    {\n",
    "        (\"name\", \"pronoun\"):[(\"Sarah\", \"her\"), (\"John\", \"His\"), (\"Martin\", \"His\"), (\"Rachel\", \"Her\")],\n",
    "        (\"city\", \"state\"): [(\"Seattle\", \"WA\"), (\"Boston\", \"MA\")],\n",
    "        \"occupation\": [\"doctor\", \"engineer\", \"teacher\", \"technician\", \"plumber\"], \n",
    "    }\n",
    ")\n",
    "\n",
    "inp = TextTemplateInput(\n",
    "    \"{name} lives in {city}, {state} and is a {occupation}. {pronoun} personal interests include\", \n",
    "    values={\"name\":\"Dave\", \"city\": \"Palm Coast\", \"state\": \"FL\", \"occupation\":\"lawyer\", \"pronoun\":\"His\"}, \n",
    "    baselines=baselines,\n",
    "    mask={\"name\":0, \"city\": 1, \"state\": 1, \"occupation\": 2, \"pronoun\": 0},\n",
    ")\n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target, num_trials=3)\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-harvard",
   "metadata": {},
   "source": [
    "One potential issue with the current approach is using Feature Ablation. If the model learns complex interations between the prompt features, the true importance may not be reflected in the attribution scores. Consider a case where the model predicts a high probability of playing golf if a person is either a lawyer or lives in Palm Coast. By ablating a feature one at a time, the probability may appear to be unchanged when ablating each feature independently, but may drop substantially when perturbing both together.\n",
    "\n",
    "To address this, we can apply alternate perturbation-based attribution methods available in Captum such as ShapleyValue(Sampling), KernelShap and Lime, which ablate different subgroups of features and may result in more accurate scores.\n",
    "\n",
    "We will use `ShapleyValue` below because we essentially only have three features now after grouping. The computation is tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = ShapleyValues(model) \n",
    "\n",
    "sv_llm_attr = LLMAttribution(sv, tokenizer)\n",
    "\n",
    "attr_res = sv_llm_attr.attribute(inp, target=target, num_trials=3)\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-america",
   "metadata": {},
   "source": [
    "Let's now consider a more complex example, where we use the LLM as a few-shot learner to classify sample movie reviews as positive or negative. We want to measure the relative impact of the few shot examples. Since the prompt changes slightly in the case that no examples are needed, we define a prompt function rather than a format string in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_fn(*examples):\n",
    "    main_prompt = \"Decide if the following movie review enclosed in quotes is Positive or Negative:\\n'I really liked the Avengers, it had a captivating plot!'\\nReply only Positive or Negative.\"\n",
    "    subset = [elem for elem in examples if elem]\n",
    "    if not subset:\n",
    "        prompt = main_prompt\n",
    "    else:\n",
    "        prefix = \"Here are some examples of movie reviews and classification of whether they were Positive or Negative:\\n\"\n",
    "        prompt = prefix + \" \\n\".join(subset) + \"\\n \" + main_prompt\n",
    "    return \"[INST] \" + prompt + \"[/INST]\"\n",
    "\n",
    "input_examples = [\n",
    "    \"'The movie was ok, the actors weren't great' Negative\", \n",
    "    \"'I loved it, it was an amazing story!' Positive\",\n",
    "    \"'Total waste of time!!' Negative\", \n",
    "    \"'Won't recommend' Negative\",\n",
    "]\n",
    "inp = TextTemplateInput(\n",
    "    prompt_fn, \n",
    "    values=input_examples,\n",
    ")\n",
    "\n",
    "attr_res = sv_llm_attr.attribute(inp)\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2739bf",
   "metadata": {},
   "source": [
    "Interestingly, we can see all these few-shot examples we choose actually make the model less likely to correctly label the given review as \"Positive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715ba4c-bd02-4e32-a9a8-f531187d5e3e",
   "metadata": {},
   "source": [
    "# Gradient-based Attribution\n",
    "As an alternative to perturbation-based attribution, we can use gradient-based methods to attribute each feature's contribution to a target sequence being generated. For LLMs, the only supported method at present is `LayerIntegratedGradients`. Layer Integrated Gradients is a variant of Integrated Gradients that assigns an importance score to layer inputs or outputs. Integrated Gradients works by assigning an importance score to each input feature by approximating the integral of gradients of the model's output with respect to the inputs along the path from given references to inputs. To instantiate, we can simply wrap our gradient-based attribution method with `LLMGradientAttribution` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf080c0a-9c51-4c1b-8ca6-a01da213a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(model, model.lm_head)\n",
    "# lig = LayerIntegratedGradients(model, model.model.embed_tokens)\n",
    "\n",
    "llm_attr = LLMGradientAttribution(lig, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f383cd-1246-4695-a96c-a0a31490cd37",
   "metadata": {},
   "source": [
    "Now that we have our LLM attribution object, we can similarly call `.attribute()` to obtain our gradient-based attributions. Right now, `LLMGradientAttribution` can only handle `TextTokenInput` inputs. We will reuse our prompt and target for this example. `LLMGradientAttribution` only supports sequence-based attribution at the moment, which we can visualize using the method `.plot_seq_attr()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121ab1b-8102-4aa9-9dc8-bd28b9c0144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTokenInput(\n",
    "    eval_prompt,\n",
    "    tokenizer,\n",
    "    skip_tokens=[1],  # skip the special token for the start of the text <s>\n",
    ")\n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target)\n",
    "\n",
    "attr_res.plot_seq_attr(show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
