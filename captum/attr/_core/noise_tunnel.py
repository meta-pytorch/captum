#!/usr/bin/env python3

import sys

import torch

from enum import Enum

from .._utils.attribution import Attribution
from .._utils.common import (
    validate_noise_tunnel_type,
    validate_input,
    format_baseline,
    format_input,
    _format_attributions,
    _format_additional_forward_args,
    _expand_additional_forward_args,
)

from .integrated_gradients import IntegratedGradients


class NoiseTunnelType(Enum):
    smoothgrad = 1
    smoothgrad_sq = 2
    vargrad = 3


SUPPORTED_ALGORITHMS_RETURNING_DELTA = [IntegratedGradients]
SUPPORTED_NOISE_TUNNEL_TYPES = list(NoiseTunnelType.__members__.keys())


class NoiseTunnel(Attribution):
    def __init__(self, attribution_method):
        r"""
        attribution_method (Attribution) An attribution algorithm such as Integrated
            Gradients, Conductance, Saliency or any other that is applied on given
            input model and inputs.
        """
        self.attribution_method = attribution_method
        self.is_delta_supported = (
            type(self.attribution_method) in SUPPORTED_ALGORITHMS_RETURNING_DELTA
        )
        super().__init__()

    def attribute(
        self, inputs, nt_type="smoothgrad", n_samples=5, noise_frac=0.2, **kwargs
    ):
        r"""
        Adds gaussian noise to each sample in the input batch `n_samples` times
        before applying attribution algorithms on the input and model.
        The resulting attribution for each sample in the batch is either
        the expected value (smoothgrad), the square of expected value (smoothgrad_sq)
        or the variance (vargrad) of all `n_samples` noisy-sample attributions per
        sample in the batch.

        More details about adding noise can me found in the following papers:
            https://arxiv.org/abs/1810.03292
            https://arxiv.org/abs/1810.03307
            https://arxiv.org/abs/1706.03825
            https://arxiv.org/pdf/1806.10758
        This method currently also supports batches of input samples, however it can
        be computationally expensive depending on the model, the dimensionality of
        the data and the environment.
        It is assumed that the batch size is the first dimension of input tensors.

            Args:

                inputs (tensor or tuple of tensors):  Input for which integrated
                            gradients are computed. If forward_func takes a single
                            tensor as input, a single input tensor should be provided.
                            If forward_func takes multiple tensors as input, a tuple
                            of the input tensors should be provided. It is assumed
                            that for all given input tensors, dimension 0 corresponds
                            to the number of examples, and if mutliple input tensors
                            are provided, the examples must be aligned appropriately.
                nt_type (string, optional): Smoothing type of the attributions.
                            `expected`, `expected_sq` or `variance`
                            Default: `smoothgrad` if `type` is not provided.
                n_samples (int, optional):  The number of randomly generated examples
                            per sample in the input batch. Random examples are
                            generated by adding gaussian random noise to each sample.
                            Default: `5` if `n_samples` is not provided.
                noise_frac (float, optional): The fraction of noise that is added
                            to each input sample. It is the fraction of
                            `sample_max` - `sample_min`. If `sample_max` is equal
                            to `sample_min` a small epsillion value is used instead.

            Return:

                attributions (tensor or tuple of tensors): Attribution with
                            respect to each input feature. attributions will always be
                            the same size as the provided inputs, with each value
                            providing the attribution of the corresponding input index.
                            If a single tensor is provided as inputs, a single tensor is
                            returned. If a tuple is provided for inputs, a tuple of
                            corresponding sized tensors is returned.
                delta (float, optional): Approximation error computed by the
                            attribution algorithm. Not all attribution algorithms
                            return delta value. It is computed only for some
                            algorithms, e.g. integrated gradients.
                            Delta is computed for each sample in the input batch
                            and represents the arithmetic mean
                            across all `n_sample` noisy samples.


            Examples::

                >>> # ImageClassifier takes a single input tensor of images Nx3x32x32,
                >>> # and returns an Nx10 tensor of class probabilities.
                >>> net = ImageClassifier()
                >>> ig = IntegratedGradients(net)
                >>> input = torch.randn(2, 3, 32, 32, requires_grad=True)
                >> # creating a noise tunnel
                >> nt = NoiseTunnel(ig)
                >>> # Generates 10-noisy samples per image,
                >>> # computes integrated gradients for class 3 for each sample
                >>> # and averages the result accros noisy samples'
                >>> # attributions per image
                >>> attribution, delta = nt.attribute(input, nt_type='smoothgrad',
                >>>                                   n_samples=10, target=3)
        """

        def add_noise_to_inputs():
            return tuple(add_noise_to_input(input) for input in inputs)

        def add_noise_to_input(input, eps=torch.tensor(sys.float_info.epsilon)):
            # batch size
            bsz = input.shape[0]

            input_flatten = input.view(bsz, -1)
            sample_max = input_flatten.max(dim=1).values
            sample_min = input_flatten.min(dim=1).values

            sample_max_min_diff = sample_max - sample_min

            # if sample_max == sample_min we use a small epsilion value
            sample_max_min_diff = torch.where(
                sample_max_min_diff == 0, eps, sample_max_min_diff
            )

            std = noise_frac * sample_max_min_diff

            # expand std for the shape of the input and number of drawn samples
            std_expanded = std.repeat_interleave(input_flatten.shape[1] * n_samples)

            # expand input size by the number of drawn samples
            input_expanded_size = (bsz * n_samples,) + input.shape[1:]

            # draws `np.prod(input_expanded_size)` samples from normal distribution
            # with given input parametrization
            noise = torch.normal(0, std_expanded).reshape(input_expanded_size)

            return input.repeat_interleave(n_samples, dim=0) + noise

        def expand_and_update_baselines():
            # TODO allow to add noise to baselines as well
            # expand baselines to match the sizes of input
            if "baselines" not in kwargs:
                return

            baselines = kwargs["baselines"]
            baselines = format_baseline(baselines, inputs)
            validate_input(inputs, baselines)

            baselines = tuple(
                baseline.repeat_interleave(n_samples, dim=0) for baseline in baselines
            )
            # update kwargs with expanded baseline
            kwargs["baselines"] = baselines

        def expand_and_update_additional_forward_args():
            if "additional_forward_args" not in kwargs:
                return
            additional_forward_args = kwargs["additional_forward_args"]
            additional_forward_args = _format_additional_forward_args(
                additional_forward_args
            )
            if additional_forward_args is None:
                return
            additional_forward_args = _expand_additional_forward_args(
                additional_forward_args, n_samples, expansion_type="repeat_interleave"
            )
            # update kwargs with expanded baseline
            kwargs["additional_forward_args"] = additional_forward_args

        def compute_expected_attribution_and_sq(attribution):
            bsz = attribution.shape[0] // n_samples
            attribution_shape = (bsz, n_samples)
            if len(attribution.shape) > 1:
                attribution_shape += attribution.shape[1:]

            attribution = attribution.view(attribution_shape)
            expected_attribution = attribution.mean(dim=1)
            expected_attribution_sq = torch.mean(attribution ** 2, dim=1)
            return expected_attribution, expected_attribution_sq

        # Keeps track whether original input is a tuple or not before
        # converting it into a tuple.
        is_inputs_tuple = isinstance(inputs, tuple)

        inputs = format_input(inputs)

        validate_noise_tunnel_type(nt_type, SUPPORTED_NOISE_TUNNEL_TYPES)

        delta_accum = 0

        inputs_with_noise = add_noise_to_inputs()
        # if the algorithm supports baselines and/or additional_forward_args they
        # will be expanded based on the n_steps and corrsponding kwargs
        # variables will be updated accordingly
        expand_and_update_baselines()
        expand_and_update_additional_forward_args()
        # smoothgrad_Attr(x) = 1 / n * sum(Attr(x + N(0, sigma^2))
        attributions = self.attribution_method.attribute(inputs_with_noise, **kwargs)
        if self.is_delta_supported:
            attributions, delta = attributions
            delta_accum += delta
        expected_attributions = []
        expected_attributions_sq = []
        for attribution in attributions:
            expected_attr, expected_attr_sq = compute_expected_attribution_and_sq(
                attribution
            )
            expected_attributions.append(expected_attr)
            expected_attributions_sq.append(expected_attr_sq)

        delta_accum /= n_samples

        if NoiseTunnelType[nt_type] == NoiseTunnelType.smoothgrad:
            return self._apply_checks_and_return_attributions(
                tuple(expected_attributions), is_inputs_tuple, delta_accum
            )

        if NoiseTunnelType[nt_type] == NoiseTunnelType.smoothgrad_sq:
            return self._apply_checks_and_return_attributions(
                tuple(expected_attributions_sq), is_inputs_tuple, delta_accum
            )

        vargrad = tuple(
            expected_attribution_sq - expected_attribution * expected_attribution
            for expected_attribution, expected_attribution_sq in zip(
                expected_attributions, expected_attributions_sq
            )
        )

        return self._apply_checks_and_return_attributions(
            vargrad, is_inputs_tuple, delta_accum
        )

    def _apply_checks_and_return_attributions(
        self, attributions, is_inputs_tuple, delta
    ):
        attributions = _format_attributions(is_inputs_tuple, attributions)

        return (attributions, delta) if self.is_delta_supported else attributions
