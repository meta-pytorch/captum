
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<div class="section" id="module-captum.attr._utils.gradient">
<span id="captum-gradient"></span><h1>Captum.Gradient<a class="headerlink" href="#module-captum.attr._utils.gradient" title="Permalink to this headline">¶</a></h1>
<dl class="function">
<dt id="captum.attr._utils.gradient.apply_gradient_requirements">
<code class="sig-prename descclassname">captum.attr._utils.gradient.</code><code class="sig-name descname">apply_gradient_requirements</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/captum/attr/_utils/gradient.html#apply_gradient_requirements"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#captum.attr._utils.gradient.apply_gradient_requirements" title="Permalink to this definition">¶</a></dt>
<dd><p>Iterates through tuple on input tensors and sets requires_grad to be true on
each Tensor, and ensures all grads are set to zero. To ensure that the input
is returned to its initial state, a list of flags representing whether or not</p>
<blockquote>
<div><p>a tensor originally required grad is returned.</p>
</div></blockquote>
</dd></dl>
<dl class="function">
<dt id="captum.attr._utils.gradient.undo_gradient_requirements">
<code class="sig-prename descclassname">captum.attr._utils.gradient.</code><code class="sig-name descname">undo_gradient_requirements</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">grad_required</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/captum/attr/_utils/gradient.html#undo_gradient_requirements"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#captum.attr._utils.gradient.undo_gradient_requirements" title="Permalink to this definition">¶</a></dt>
<dd><p>Iterates through list of tensors, zeros each gradient, and sets required
grad to false if the corresponding index in grad_required is False.
This method is used to undo the effects of prepare_gradient_inputs, making
grads not required for any input tensor that did not initially require
gradients.</p>
</dd></dl>
<dl class="function">
<dt id="captum.attr._utils.gradient.compute_gradients">
<code class="sig-prename descclassname">captum.attr._utils.gradient.</code><code class="sig-name descname">compute_gradients</code><span class="sig-paren">(</span><em class="sig-param">forward_fn</em>, <em class="sig-param">inputs</em>, <em class="sig-param">target_ind=None</em>, <em class="sig-param">additional_forward_args=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/captum/attr/_utils/gradient.html#compute_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#captum.attr._utils.gradient.compute_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes gradients of the output with respect to inputs for an
arbitrary forward function.</p>
<p>Args</p>
<blockquote>
<div><dl class="simple">
<dt>forward_fn: forward function. This can be for example model’s</dt><dd><p>forward function.</p>
</dd>
<dt>input:      Input at which gradients are evaluated,</dt><dd><p>will be passed to forward_fn.</p>
</dd>
<dt>target_ind: Index of the target class for which gradients</dt><dd><p>must be computed (classification only).</p>
</dd>
<dt>args:       Additional input arguments that forward function requires.</dt><dd><p>It takes an empty tuple (no additional arguments) if no
additional arguments are required</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>
<dl class="function">
<dt id="captum.attr._utils.gradient.compute_layer_gradients_and_eval">
<code class="sig-prename descclassname">captum.attr._utils.gradient.</code><code class="sig-name descname">compute_layer_gradients_and_eval</code><span class="sig-paren">(</span><em class="sig-param">forward_fn</em>, <em class="sig-param">layer</em>, <em class="sig-param">inputs</em>, <em class="sig-param">target_ind=None</em>, <em class="sig-param">additional_forward_args=None</em>, <em class="sig-param">gradient_neuron_index=None</em>, <em class="sig-param">device_ids=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/captum/attr/_utils/gradient.html#compute_layer_gradients_and_eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#captum.attr._utils.gradient.compute_layer_gradients_and_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes gradients of the output with respect to a given layer as well
as the output evaluation of the layer for an arbitrary forward function
and given input.</p>
<p>For data parallel models, hooks are executed once per device ,so we
need to internally combine the separated tensors from devices by
concatenating based on device_ids. Any necessary gradients must be taken
with respect to each independent batched tensor, so the gradients are
computed and combined appropriately.</p>
<p>More information regarding the behavior of forward hooks with DataParallel
models can be found in the PyTorch data parallel documentation. We maintain
the separate inputs in a dictionary protected by a lock, analogous to the
gather implementation for the core PyTorch DataParallel implementation.</p>
<p>Args</p>
<blockquote>
<div><dl class="simple">
<dt>forward_fn: forward function. This can be for example model’s</dt><dd><p>forward function.</p>
</dd>
</dl>
<p>layer:      Layer for which gradients / output will be evaluated.
inputs:     Input at which gradients are evaluated,</p>
<blockquote>
<div><p>will be passed to forward_fn.</p>
</div></blockquote>
<dl class="simple">
<dt>target_ind: Index of the target class for which gradients</dt><dd><p>must be computed (classification only).</p>
</dd>
<dt>args:       Additional input arguments that forward function requires.</dt><dd><p>It takes an empty tuple (no additional arguments) if no
additional arguments are required</p>
</dd>
</dl>
</div></blockquote>
<p>Return</p>
<blockquote>
<div><p>gradients:  Gradients of output with respect to target layer output.
evals:      Target layer output for given input.</p>
</div></blockquote>
</dd></dl>
</div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Captum</a></h1>
<h3>Navigation</h3>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="deep_lift.html">Captum.DeepLift</a></li>
<li class="toctree-l1"><a class="reference internal" href="input_x_gradient.html">Captum.InputXGradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="integrated_gradients.html">Captum.IntegratedGradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal_influence.html">Captum.InternalInfluence</a></li>
<li class="toctree-l1"><a class="reference internal" href="layer_activation.html">Captum.LayerActivation</a></li>
<li class="toctree-l1"><a class="reference internal" href="layer_conductance.html">Captum.LayerConductance</a></li>
<li class="toctree-l1"><a class="reference internal" href="layer_gradient_x_activation.html">Captum.LayerGradientXActivation</a></li>
<li class="toctree-l1"><a class="reference internal" href="neuron_conductance.html">Captum.NeuronConductance</a></li>
<li class="toctree-l1"><a class="reference internal" href="neuron_gradient.html">Captum.NeuronGradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="neuron_integrated_gradients.html">Captum.NeuronIntegratedGradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="noise_tunnel.html">Captum.NoiseTunnel</a></li>
<li class="toctree-l1"><a class="reference internal" href="saliency.html">Captum.Saliency</a></li>
<li class="toctree-l1"><a class="reference internal" href="base.html">Captum.Models.Base</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytext.html">Captum.Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="approximation_methods.html">Captum Approximation</a></li>
<li class="toctree-l1"><a class="reference internal" href="attribution.html">Captum.Utils.Attribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="common.html">Captum.Utils</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Captum.Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">Captum.Visualization</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="index.html">Documentation overview</a><ul>
<li>Previous: <a href="common.html" title="previous chapter">Captum.Utils</a></li>
<li>Next: <a href="visualization.html" title="next chapter">Captum.Visualization</a></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div>